---
title: "MLP"
output: html_document
date: "2022-12-03"
---

```{r}
library(fastDummies)
library(readr)
library(palmerpenguins)

```


```{r}
sigmoid <- function(x){
  return(1 / (1 + exp(-x)))
}

softmax <- function(x){
  return(exp(x) / sum(exp(x)))
}
#Relu <- function(x){
 # return(max(x,0)) 
#}
relu <- function(x) sapply(x, function(z) max(0,z))

```



```{r}
data(package = 'palmerpenguins')

data("penguins")
df <- penguins
head(df)


```


```{r}

#drop nan values
df <- df[complete.cases(df), ] 
# Create dummy variable
df <- dummy_cols(df, select_columns = "island")
df <- dummy_cols(df, select_columns = "sex")
# Remove  Columns in List
df <- df[,!names(df) %in% c("island", "sex")]
```



```{r}
#Randomly shuffle the dataset rows (repeatedly shuffled for 5 times)
rows_count <- nrow(df)
for(k in 1:5){
  df <-df[sample(rows_count),]
}

```



```{r}
#scale data
df[c(2,3,4,5,6)] <- lapply(df[c(2,3,4,5,6)], function(x) c(scale(x)))
validation_instances <- sample(nrow(df)/5)
test <- df[validation_instances,] #1/3 rd validation set
train <- df[-validation_instances,] #2/3 rd training set

X_train <- train[, -1]
X_test <- test[, -1]

y_train <- model.matrix(~ species-1, data = train)
y_test <- model.matrix(~ species-1, data = test)
X_train <- as.matrix(X_train, byrow=TRUE)

X_train <- t(X_train)
y_train <- t(y_train)

X_test <- as.matrix(X_test, byrow=TRUE)
X_test <- t(X_test)
y_test <- t(y_test)

```


get the different parameters of layer size

```{r}
getLayerSize <- function(X, y, hidden_neurons, train=TRUE) {
  n_x <- dim(X)[1]
  n_h <- hidden_neurons
  n_y <- dim(y)[1]  
  
  size <- list("n_x" = n_x,
               "n_h" = n_h,
               "n_y" = n_y)
  
  return(size)
}
```


Initialize Parameters

```{r}
initializeParameters <- function(X, list_layer_size){
  
  m <- dim(data.matrix(X))[2]
  
  n_x <- list_layer_size$n_x
  n_h <- list_layer_size$n_h
  n_y <- list_layer_size$n_y
  
  W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.001
  b1 <- matrix(rep(0, n_h), nrow = n_h)
  W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.001
  b2 <- matrix(rep(0, n_y), nrow = n_y)
  
  params <- list("W1" = W1,
                 "b1" = b1, 
                 "W2" = W2,
                 "b2" = b2)
  
  return (params)
}

```

Forwardpropagation

```{r}
forwardPropagation <- function(X, params, list_layer_size){
  
  m <- dim(X)[2]
  n_h <- list_layer_size$n_h
  n_y <- list_layer_size$n_y
  
  W1 <- params$W1
  b1 <- params$b1
  W2 <- params$W2
  b2 <- params$b2
  
  b1_new <- matrix(rep(b1, m), nrow = n_h)
  b2_new <- matrix(rep(b2, m), nrow = n_y)

  
  Z1 <- W1 %*% X + b1_new
  A1 <- Z1
  Z2 <- W2 %*% A1 + b2_new
  A2 <- sigmoid(Z2)
  
  cache <- list("Z1" = Z1,
                "A1" = A1, 
                "Z2" = Z2,
                "A2" = A2)
  
  return (cache)
}
```

calculate the cost function

```{r}

computeCost <- function(X, y, cache) {
  m <- dim(X)[2]
  A2 <- cache$A2
  logprobs <- (log(A2) * y) + (log(1-A2) * (1-y))
  cost <- -sum(logprobs/m) 
  return (cost)
}

```

backward Propagation

```{r}
backwardPropagation <- function(X, y, cache, params, list_layer_size){
  
  m <- dim(X)[2]
  
  n_x <- list_layer_size$n_x
  n_h <- list_layer_size$n_h
  n_y <- list_layer_size$n_y
  
  A2 <- cache$A2
  A1 <- cache$A1
  W2 <- params$W2
  
  dZ2 <- A2 - y
  dW2 <- 1/m * (dZ2 %*% t(A1)) 
  db2 <- matrix(1/m * sum(dZ2), nrow = n_y)
  db2_new <- matrix(rep(db2, m), nrow = n_y)
  
  dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
  dW1 <- 1/m * (dZ1 %*% t(X))
  db1 <- matrix(1/m * sum(dZ1), nrow = n_h)
  db1_new <- matrix(rep(db1, m), nrow = n_h)
  
  grads <- list("dW1" = dW1, 
                "db1" = db1,
                "dW2" = dW2,
                "db2" = db2)
  
  return(grads)
}
```
update Parameters

```{r}
updateParameters <- function(grads, params, learning_rate){
  
  W1 <- params$W1
  b1 <- params$b1
  W2 <- params$W2
  b2 <- params$b2
  
  dW1 <- grads$dW1
  db1 <-  grads$db1
  dW2 <-  grads$dW2
  db2 <-  grads$db2
  
  
  W1 <- W1 - learning_rate * dW1
  b1 <- b1 - learning_rate * db1
  W2 <- W2 - learning_rate * dW2
  b2 <- b2 - learning_rate * db2
  
  updated_params <- list("W1" = W1,
                         "b1" = b1,
                         "W2" = W2,
                         "b2" = b2)
  
  return (updated_params)
}
```

Train the model

```{r}
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
  
  layer_size <- getLayerSize(X, y, hidden_neurons)
  init_params <- initializeParameters(X, layer_size)
  cost_history <- c()
  for (i in 1:num_iteration) {
    fwd_prop <- forwardPropagation(X, init_params, layer_size)
    cost <- computeCost(X, y, fwd_prop)
    back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
    update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
    init_params <- update_params
    cost_history <- c(cost_history, cost)
    
    if (i %% 1000 == 0) cat("Iteration", i, " | Cost: ", cost, "\n")
  }
  
  model_out <- list("updated_params" = update_params,
                    "cost_hist" = cost_history)
  return (model_out)
}
```


```{r}
EPOCHS = 40000
HIDDEN_NEURONS = 50
LEARNING_RATE = 0.002

```

```{r}
train_model <- trainModel(X_train, y_train, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)

```
create a function to Make predictions

```{r}
makePrediction <- function(X, y, hidden_neurons){
  layer_size <- getLayerSize(X, y, hidden_neurons)
  params <- train_model$updated_params
  fwd_prop <- forwardPropagation(X, params, layer_size)
  pred <- fwd_prop$A2
  
  return (pred)
}
```

store the prediction of X_test in y_pred and the predictions of X_train in ypredtrain

```{r}
y_pred <- makePrediction(X_test, y_test, HIDDEN_NEURONS)
ypredtrain <- makePrediction(X_train, y_train, HIDDEN_NEURONS)

```


convert probabilities into classes
class 1 for 'speciesAdelie' class 2 for 'speciesChinstrap' and finally class 3 for category 'speciesGentoo'

```{r}
yy <- function(ypred){
  ypred <- as.data.frame(t(ypred)) 
  ypredected <- c()
  for (i in 1:dim(ypred)[1]) {
    aa <- max(ypred[i,])
    if ( aa == ypred[i,][1]) {
    ypredected <- c(ypredected, 1)
    } else if ( aa == ypred[i,][2]) {
    ypredected <- c(ypredected, 2)
    } else {
    ypredected <- c(ypredected, 3)
    }
 
  }
  return(ypredected)
}
```


```{r}
y_true <- yy(y_test)
y_pred <- yy(y_pred)
ypredtrain <- yy(ypredtrain)
ytruetrain <- yy(y_train)

```

Plotting the cost function 
as we can see its converging at the end

```{r}
plot(1:EPOCHS, train_model$cost_hist, type = 'l')
```
Confusion matrix for X_test

```{r}

table(y_pred, y_true)
```

confusion matrix for X_train

```{r}
table(ypredtrain, ytruetrain)

```

