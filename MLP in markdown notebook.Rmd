---
title: "MLP"
output: html_document
date: "2022-12-03"
---

```{r}
library(fastDummies)
library(readr)
library(palmerpenguins)
sigmoid <- function(x){
1 / (1 + exp(-x))
}

softmax <- function(x){
exp_x <- exp(x)
exp_x / matrix(colSums(exp_x), nrow = nrow(x), ncol = ncol(x), byrow = TRUE)
}

relu <- function(x) sapply(x, function(z) max(0, z))
data(package = 'palmerpenguins')
data("penguins")
df <- penguins
df <- df[complete.cases(df), ]
df <- dummy_cols(df, select_columns = "island")
df <- dummy_cols(df, select_columns = "sex")
df <- df[, !names(df) %in% c("island", "sex")]
rows_count <- nrow(df)
for (k in 1:5) {
df <- df[sample(rows_count), ]
}
df[c(2,3,4,5,6)] <- lapply(df[c(2,3,4,5,6)], scale)

validation_instances <- sample(nrow(df), nrow(df)/5)
test <- df[validation_instances, ]
train <- df[-validation_instances, ]

X_train <- t(as.matrix(train[, -1]))
X_test  <- t(as.matrix(test[, -1]))

y_train <- t(model.matrix(~ species - 1, data = train))
y_test  <- t(model.matrix(~ species - 1, data = test))
getLayerSize <- function(X, y, hidden_neurons){
list(
n_x = nrow(X),
n_h = hidden_neurons,
n_y = nrow(y)
)
}
initializeParameters <- function(layer_size){
W1 <- matrix(runif(layer_size$n_h * layer_size$n_x),
nrow = layer_size$n_h) * 0.01
b1 <- matrix(0, nrow = layer_size$n_h, ncol = 1)

W2 <- matrix(runif(layer_size$n_y * layer_size$n_h),
nrow = layer_size$n_y) * 0.01
b2 <- matrix(0, nrow = layer_size$n_y, ncol = 1)

list(W1 = W1, b1 = b1, W2 = W2, b2 = b2)
}
forwardPropagation <- function(X, params){
m <- ncol(X)

Z1 <- params$W1 %*% X + matrix(rep(params$b1, m), nrow = nrow(params$b1))
A1 <- tanh(Z1)

Z2 <- params$W2 %*% A1 + matrix(rep(params$b2, m), nrow = nrow(params$b2))
A2 <- softmax(Z2)

list(Z1 = Z1, A1 = A1, Z2 = Z2, A2 = A2)
}
computeCost <- function(y, A2){
m <- ncol(y)
-sum(y * log(A2 + 1e-8)) / m
}
backwardPropagation <- function(X, y, cache, params){
m <- ncol(X)

dZ2 <- cache$A2 - y
dW2 <- (1/m) * dZ2 %*% t(cache$A1)
db2 <- matrix((1/m) * rowSums(dZ2), nrow = nrow(dZ2))

dZ1 <- (t(params$W2) %*% dZ2) * (1 - cache$A1^2)
dW1 <- (1/m) * dZ1 %*% t(X)
db1 <- matrix((1/m) * rowSums(dZ1), nrow = nrow(dZ1))

list(dW1 = dW1, db1 = db1, dW2 = dW2, db2 = db2)
}
updateParameters <- function(params, grads, lr){
params$W1 <- params$W1 - lr * grads$dW1
params$b1 <- params$b1 - lr * grads$db1
params$W2 <- params$W2 - lr * grads$dW2
params$b2 <- params$b2 - lr * grads$db2
params
}
trainModel <- function(X, y, epochs, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
params <- initializeParameters(layer_size)
cost_history <- c()

for (i in 1:epochs) {
cache <- forwardPropagation(X, params)
cost <- computeCost(y, cache$A2)
grads <- backwardPropagation(X, y, cache, params)
params <- updateParameters(params, grads, lr)
cost_history <- c(cost_history, cost)

if (i %% 1000 == 0) {
  cat("Iteration", i, "| Cost:", cost, "\n")
}

}

list(params = params, cost_hist = cost_history)
}
EPOCHS <- 40000
HIDDEN_NEURONS <- 50
LEARNING_RATE <- 0.002
train_model <- trainModel(
X_train,
y_train,
epochs = EPOCHS,
hidden_neurons = HIDDEN_NEURONS,
lr = LEARNING_RATE
)
makePrediction <- function(X, params){
cache <- forwardPropagation(X, params)
cache$A2
}
y_pred_test  <- makePrediction(X_test,  train_model$params)
y_pred_train <- makePrediction(X_train, train_model$params)
toClass <- function(probs){
apply(probs, 2, which.max)
}

y_true_test  <- toClass(y_test)
y_pred_test  <- toClass(y_pred_test)

y_true_train <- toClass(y_train)
y_pred_train <- toClass(y_pred_train)
plot(1:EPOCHS, train_model$cost_hist, type = "l")
table(y_pred_test, y_true_test)
table(y_pred_train, y_true_train)
